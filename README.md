# NeuroDoc System - Production-Grade Document Processing Backend

A complete document processing system with AI-powered analysis, built with Next.js 13+ App Router, Supabase, and OpenAI.

## üéØ Features

- **Document Upload & Processing**: PDF, DOCX, EML support with text extraction
- **AI-Powered Analysis**: OpenAI embeddings for semantic search and GPT-4 for decision making
- **Vector Database**: Supabase with pgvector for similarity search
- **Production Ready**: JWT authentication, Row Level Security, comprehensive API
- **Real-time Decisions**: Returns structured JSON with confidence scores and justifications

## üõ† Tech Stack

- **Frontend**: Next.js 13+ (App Router), TypeScript, Tailwind CSS, ShadCN UI
- **Backend**: Next.js API Routes
- **Database**: Supabase PostgreSQL with pgvector extension
- **AI/ML**: OpenAI GPT-4 and text-embedding-3-small
- **Auth**: Supabase Auth (JWT)
- **Storage**: Supabase Storage
- **Document Processing**: pdf-parse, mammoth (DOCX), custom EML parser

## üì¶ Installation & Setup

### 1. Clone and Install Dependencies

```bash
cd neurodoc-system
npm install --legacy-peer-deps
```

### 2. Environment Configuration

Copy `.env.local` and fill in your credentials:

```bash
# Supabase Configuration
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key

# NextJS Configuration
NEXTAUTH_SECRET=your_nextauth_secret
NEXTAUTH_URL=http://localhost:3000

# File Upload Configuration
MAX_FILE_SIZE=10485760
ALLOWED_FILE_TYPES=pdf,docx,eml
```

### 3. Supabase Setup

1. Create a new Supabase project
2. Enable the pgvector extension in the SQL Editor:
   ```sql
   CREATE EXTENSION vector;
   ```
3. Run the database schema from `database/schema.sql`
4. Create a storage bucket named "documents"

### 4. Run the Development Server

```bash
npm run dev
```

The application will be available at `http://localhost:3000`

## üîå API Endpoints

### Authentication
All endpoints require JWT authentication via `Authorization: Bearer <token>` header.

### 1. Upload Documents
```http
POST /api/upload
Content-Type: multipart/form-data

{
  "file": <PDF/DOCX/EML file>
}
```

**Response:**
```json
{
  "success": true,
  "document": {
    "id": "uuid",
    "fileName": "document.pdf",
    "fileType": "pdf",
    "chunks": 15,
    "pages": 5,
    "uploadedAt": "2025-01-01T00:00:00Z"
  }
}
```

### 2. Query Documents
```http
POST /api/query
Content-Type: application/json

{
  "query": "46M, knee surgery in Pune, 3-month policy",
  "options": {
    "limit": 5,
    "threshold": 0.7,
    "includeReferences": true
  }
}
```

**Response:**
```json
{
  "Decision": "Approved",
  "Amount": "‚Çπ75,000",
  "Justification": {
    "ClauseReference": "Policy_2023.pdf > Page 12 > Section 4.2",
    "ClauseText": "Knee surgeries are covered after 90 days."
  },
  "ConfidenceScore": 0.92,
  "QueryId": "uuid",
  "SearchResults": [...]
}
```

### 3. Get Documents
```http
GET /api/documents?limit=50&offset=0&search=policy&fileType=pdf
```

### 4. Get Document Clauses
```http
GET /api/clauses?doc_id=uuid&limit=50&include_embeddings=false
```

### 5. Audit Trail
```http
GET /api/audit?limit=20&start_date=2025-01-01&end_date=2025-01-31
```

**Audit Statistics:**
```http
POST /api/audit
Content-Type: application/json

{
  "startDate": "2025-01-01",
  "endDate": "2025-01-31",
  "groupBy": "day"
}
```

## üóÑ Database Schema

### Core Tables

1. **documents** - Uploaded file metadata
2. **clauses** - Document chunks with vector embeddings
3. **queries** - User queries and AI decisions
4. **audit_log** - Decision trail and clause references

### Key Features

- **Row Level Security (RLS)** - Users can only access their own data
- **Vector Search** - Optimized similarity search with pgvector
- **JSONB Storage** - Flexible decision storage
- **Comprehensive Indexing** - Optimized for performance

## üîç Document Processing Pipeline

1. **Upload**: File validation and storage in Supabase Storage
2. **Extract**: Text extraction based on file type (PDF/DOCX/EML)
3. **Chunk**: Text segmentation with overlap for better context
4. **Embed**: OpenAI embeddings generation for each chunk
5. **Store**: Vector storage in PostgreSQL with metadata
6. **Index**: Automatic indexing for fast retrieval

## ü§ñ AI Decision Pipeline

1. **Query Processing**: Natural language query acceptance
2. **Vector Search**: Semantic similarity search across document clauses
3. **Context Building**: Relevant clauses compilation with metadata
4. **LLM Analysis**: GPT-4 analysis with structured prompt
5. **Decision Generation**: JSON response with justification
6. **Audit Logging**: Complete decision trail storage

## üìä Example Usage

### Upload a Policy Document
```bash
curl -X POST "http://localhost:3000/api/upload" \
  -H "Authorization: Bearer your_jwt_token" \
  -F "file=@policy_document.pdf"
```

### Query for Coverage Decision
```bash
curl -X POST "http://localhost:3000/api/query" \
  -H "Authorization: Bearer your_jwt_token" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "35-year-old male, cardiac surgery, premium member",
    "options": {
      "limit": 10,
      "threshold": 0.75
    }
  }'
```

### Get Audit Trail
```bash
curl "http://localhost:3000/api/audit?limit=10" \
  -H "Authorization: Bearer your_jwt_token"
```

## üöÄ Production Deployment

### Environment Variables
Ensure all production environment variables are set:
- Use production Supabase instance
- Set secure `NEXTAUTH_SECRET`
- Configure proper CORS settings
- Set production OpenAI API limits

### Security Considerations
- Enable Supabase RLS policies
- Configure proper CORS headers
- Implement rate limiting
- Use HTTPS in production
- Validate file uploads thoroughly

### Performance Optimization
- Enable pgvector indexing
- Configure proper connection pooling
- Implement caching where appropriate
- Monitor OpenAI API usage
- Set up error tracking (Sentry, etc.)

## üîß Configuration Options

### File Upload Limits
```env
MAX_FILE_SIZE=10485760  # 10MB
ALLOWED_FILE_TYPES=pdf,docx,eml
```

### AI Model Configuration
```typescript
// In lib/openai.ts
export const EMBEDDING_MODEL = 'text-embedding-3-small'
export const CHAT_MODEL = 'gpt-4'
export const EMBEDDING_DIMENSIONS = 1536
```

### Vector Search Tuning
```sql
-- Adjust the lists parameter based on your data size
CREATE INDEX idx_clauses_embedding ON clauses 
USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);
```

## üìà Monitoring & Analytics

The system includes comprehensive audit logging and statistics:

- Query success rates
- Confidence score distributions
- Document usage analytics
- Performance metrics
- Error tracking

Access via the `/api/audit` endpoint for detailed analytics.

## ü§ù Integration

### External API Access
The system is designed to be easily integrated with external systems:

```javascript
// Example integration
const response = await fetch('https://your-domain.com/api/query', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${your_api_token}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    query: 'Your query here',
    options: { limit: 5, threshold: 0.7 }
  })
});

const decision = await response.json();
```

### Webhook Support
Extend the system with webhooks for real-time notifications:

```typescript
// Add to your API routes
const webhook = await fetch('https://your-webhook-url.com', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    event: 'decision_made',
    data: decision
  })
});
```

## üêõ Troubleshooting

### Common Issues

1. **Vector Search Not Working**
   - Ensure pgvector extension is enabled
   - Check embedding dimensions match (1536)
   - Verify index creation

2. **File Upload Failures**
   - Check file size limits
   - Verify Supabase storage permissions
   - Ensure file types are allowed

3. **Authentication Issues**
   - Verify JWT token format
   - Check Supabase Auth configuration
   - Ensure RLS policies are correct

### Debug Mode
Enable verbose logging by setting:
```env
NODE_ENV=development
DEBUG=neurodoc:*
```

## üìù License

This project is designed for production use. Ensure compliance with OpenAI's terms of service and data handling regulations.

---

Built with ‚ù§Ô∏è for intelligent document processing.
